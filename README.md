# تحلیل احساسات متن با Bag-of-Words و Naive Bayes

این پروژه یک پیاده‌سازی کلاسیک در پردازش زبان طبیعی (NLP) برای طبقه‌بندی احساسات (Sentiment Analysis) است. هدف، ساخت مدلی است که بتواند نظرات کاربران در مورد یک رستوران را بخواند و آن را به عنوان **مثبت (Positive)** یا **منفی (Negative)** با دقت (79%) طبقه‌بندی کند.

این پروژه یک گردش کار بنیادی یادگیری ماشین را به نمایش می‌گذارد: پیش‌پردازش متن، استخراج ویژگی با استفاده از مدل **Bag-of-Words (BoW)**، و آموزش یک طبقه‌بند آماری (`Naive Bayes`).

---

## دیتاست (Dataset)

دیتاست مورد استفاده فایل `Restaurant_Reviews.tsv` است، مجموعه‌ای از ۱۰۰۰ نظر که با tab از هم جدا شده‌اند. این دیتاست شامل دو ستون کلیدی است:
* **`Review`**: متن نظر مشتری.
* **`Liked`**: برچسب احساسات ( `1` برای مثبت، `0` برای منفی).

---

## متدولوژی و فرآیند (Methodology)

این پروژه در سه مرحله‌ی اصلی انجام شده است:

### ۱. پیش‌پردازش و پاک‌سازی متن
قبل از اینکه متن بتواند توسط مدل درک شود، باید پاک‌سازی شود. یک «مجموعه» (Corpus) از متون پاک‌سازی‌شده با طی کردن مراحل زیر برای هر نظر ساخته شد:

1.  **حذف علائم نگارشی و اعداد**: با استفاده از عبارات باقاعده (Regex)، تمام کاراکترهای غیر الفبایی (`[^a-zA-Z]`) حذف شدند.
2.  **استانداردسازی**: تمام متن به حروف کوچک تبدیل شد.
3.  **حذف کلمات توقف (Stopwords)**: کلمات رایج و بی‌اهمیت انگلیسی (مانند `the`, `is`, `in`) با استفاده از لیست `stopwords` کتابخانه‌ی `NLTK` حذف شدند.
4.  **یک بهینه‌سازی کلیدی**: کلمه‌ی **`not`** (نه) به طور خاص از لیست کلمات توقف **حذف شد**. این کار حیاتی است، زیرا `not` معنای یک نظر را کاملاً برعکس می‌کند (مثلاً "not good") و برای تشخیص احساسات اهمیت دارد.
5.  **ریشه‌یابی (Stemming)**: هر کلمه به ریشه‌ی خود کاهش یافت (مثلاً `loving` به `love` تبدیل شد) تا از پراکندگی واژگان کاسته شود.

### ۲. استخراج ویژگی (Bag-of-Words)
مجموعه‌ی متون پاک‌سازی‌شده با استفاده از مدل Bag-of-Words (BoW) به یک ماتریس عددی تبدیل شد:

* از `CountVectorizer` کتابخانه‌ی `Scikit-learn` برای ساخت واژگان و شمارش فراوانی هر کلمه استفاده شد.
* برای کنترل ابعاد و جلوگیری از نویز، واژگان به **۱۵۰۰ کلمه‌ی پرتکرار** (`max_features = 1500`) محدود شد.
* خروجی، یک ماتریس اسپارس بود که در آن هر سطر یک نظر و هر ستون، شمارش یکی از ۱۵۰۰ کلمه‌ی برتر است.

### ۳. آموزش و ارزیابی مدل
1.  دیتاست با نسبت **۸۰٪ آموزشی** و **۲۰٪ آزمایشی** (`test_size = 0.20`) تقسیم شد. (`random_state = 0` برای اطمینان از تکرارپذیری نتایج استفاده شد).
2.  یک مدل **Gaussian Naive Bayes** (`GaussianNB`) بر روی داده‌های آموزشی، آموزش داده شد. این طبقه‌بند به دلیل سادگی و عملکرد خوب بر روی دیتاست‌های BoW انتخاب شد.

---

## نتایج و ارزیابی (Results)

عملکرد مدل بر روی ۲۰۰ نمونه‌ی آزمایشی (۲۰٪ داده‌ها) ارزیابی شد و به نتایج زیر دست یافت:
[[79 18] [24 79]]

* **True Negatives (TN): 79** (۷۹ نظر منفی را به درستی «منفی» تشخیص داد)
* **False Positives (FP): 18** (۱۸ نظر منفی را به اشتباه «مثبت» تشخیص داد)
* **False Negatives (FN): 24** (۲۴ نظر مثبت را به اشتباه «منفی» تشخیص داد)
* **True Positives (TP): 79** (۷۹ نظر مثبت را به درستی «مثبت» تشخیص داد)

---

## ابزارها و کتابخانه‌های استفاده‌شده
* **Python 3.x**
* **Pandas**: برای بارگذاری و مدیریت داده‌ها
* **NLTK**: برای پاک‌سازی متن، مدیریت stopwords و ریشه‌یابی (PorterStemmer)
* **Scikit-learn (`sklearn`)**: برای `CountVectorizer`، `train_test_split`، مدل `GaussianNB` و معیارهای ارزیابی (`confusion_matrix`, `accuracy_score`)
* **Numpy**: برای عملیات‌های عددی

---

## نحوه اجرا (How to Run)

1.  این ریپازیتوری را `clone` کنید:
    ```bash
    git clone [https://github.com/erfan-8/BoW-Sentiment-Classifier.git](https://github.com/erfan-8/BoW-Sentiment-Classifier.git)
    ```
2.  وارد پوشه‌ی پروژه شوید:
    ```bash
    cd BoW-Sentiment-Classifier
    ```
3.  کتابخانه‌های مورد نیاز را نصب کنید:
    ```bash
    pip install numpy pandas matplotlib nltk scikit-learn
    ```
4.  (فقط برای بار اول) بسته‌ی `stopwords` کتابخانه‌ی NLTK را دانلود کنید. یک اسکریپت پایتون باز کنید و اجرا کنید:
    ```python
    import nltk
    nltk.download('stopwords')
    ```
5.  اسکریپت اصلی (`natural_language_processing.py`) یا نوت‌بوک جوپیتر (`.ipynb`) را اجرا کنید تا مدل آموزش ببیند و نتایج در ترمینال چاپ شوند.
**دقت نهایی (Accuracy): 79.0%**

### ماتریس درهم‌ریختگی (Confusion Matrix):
